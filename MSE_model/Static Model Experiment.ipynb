{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Static Model Experiment.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_1rb4QPrmKDB"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"R_NVoxTKmKDC"},"source":["This Static Model takes the CMAQ data as input and generates .npy downscaled data based on the given resolution which can be edited below. This resolution MUST be a multiple of 356."]},{"cell_type":"code","metadata":{"id":"WmJjkXzXmKDD"},"source":["import numpy as np\n","import math\n","import os.path\n","from os import path\n","import pandas as pd\n","from scipy.spatial.distance import cdist\n","from scipy.stats import invgamma\n","\n","%matplotlib inline\n","\n","global resolution\n","global w_h\n","global s\n","global date\n","\n","# 356x356 km grid\n","resolution = 4            # default: 4 km resolution\n","w_h = int(356/resolution) # default: 89x89 grid\n","s = int(w_h**2)           # default: 7921 sites\n","date = '12/30/16'           # default: 1/1/16\n","\n","data = list(range(s))\n","coords_of_sites = list(range(s))\n","CMAQ_offset = list(range(s))\n","\n","def progressBar(current, total, barLength):\n","    percent = float(current) * 100 / total\n","    arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n","    spaces  = ' ' * (barLength - len(arrow))\n","\n","    print('Progress: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sc7zmrlYmKDE"},"source":["Estimations below are derived from Berrocal et. al trace plots.\n","We assume the multiplicative bias of the numerical model output is constant in space.\n","Thus, in fitting the static downscalter, we set the local adjustment Beta_1(s) to zero."]},{"cell_type":"code","metadata":{"id":"opP_oLFamKDE"},"source":["#overall additive and multiplicative bias of CMAQ\n","beta_0, beta_1 = 3.2, 0.5\n","\n","#nugget variance for white noise process\n","tau_2 = 0.10\n","\n","#spatial decay parameter for Gaussian process wj(s)\n","phi_0 = .003\n","\n","#relevant elements of matrix A\n","A_11 = 1.1\n","#this will be the value we predict in our model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IwTR6RePmKDE"},"source":["First, we define the Gaussian Process for the model. This can take a LONG time since the covariance matrix is s^2/s^2, so we have the ability to load a previous matrix as an .npy file and we always save our new one."]},{"cell_type":"code","metadata":{"id":"AUVcwkO0mKDF"},"source":["def kernel(phi=-.003):\n","    \"\"\"\n","    Exponential kernel with spatial decay parameter phi.\n","    Default resolution=4km, i.e. 89x89 grid.\n","    \"\"\"\n","    \n","    global resolution\n","    global w_h\n","    global s\n","    \n","    # checks for saved covariance matrix\n","    if path.exists(\"covariance_\" + str(resolution) + \".npy\"):\n","        with open(\"covariance_\" + str(resolution) + \".npy\", 'rb') as f:\n","            c = np.load(f, allow_pickle=False)\n","            return c\n","\n","    # generates covariance matrix\n","    \n","    print(\"Generating coordinates for %d sites...\" % (s))\n","    s_coords = [[0 for i in range(w_h)] for j in range(w_h)] \n","    for x in range(w_h):\n","        for y in range(w_h):\n","            s_coords[x][y] = [x, y]     \n","    \n","    print(\"Generating %s x %s empty matrix...\" % (s, s))\n","    cov = list(range(s))\n","    for i in range(s):\n","        progressBar(i, s, barLength = 80)\n","        cov[i] = list(range(s))\n","        \n","    print(\"Generating %s x %s covariance matrix...\" % (s, s))\n","    for y in range(s):\n","        progressBar(y, s, barLength = 80)\n","        for x in range(s):\n","            cov[x][y] = np.exp(phi * math.sqrt((s_coords[y%w_h][y//w_h][0] - s_coords[x%w_h][x//w_h][0])**2 + \\\n","                                               (s_coords[y%w_h][y//w_h][1] - s_coords[x%w_h][x//w_h][1])**2))\n","    \n","    # saves and returns new covariance matrix\n","    with open(\"covariance_\" + str(resolution) + \".npy\", 'wb') as f:\n","        np.save(f, cov, allow_pickle=False)\n","        return cov"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mys-3swFmKDF"},"source":["Now we can generate a s-sized sample from the multivariate normal. Similar to the covariance matrix, this can take a long time, so we include the ability to load or save."]},{"cell_type":"code","metadata":{"id":"smNDb0B3mKDF"},"source":["# checks for saved sample\n","if path.exists(\"sample_\" + str(resolution) + \".npy\"):\n","    with open(\"sample_\" + str(resolution) + \".npy\", 'rb') as f:\n","        sample = np.load(f, allow_pickle=False)\n","\n","# otherwise, generates new sample\n","else:\n","    with open(\"sample_\" + str(resolution) + \".npy\", 'wb') as f:\n","        mu = np.zeros(s)\n","        cov = kernel()\n","        print(\"Generating sample...\")\n","        sample = np.random.multivariate_normal(mu, cov, 1)\n","        sample = sample[0]\n","        np.save(f, sample, allow_pickle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v7wzb66bmKDG"},"source":["Now we can start to work with the CMAQ data."]},{"cell_type":"code","metadata":{"id":"55Wvaz23mKDG"},"source":["# import and clean dataset\n","CMAQ_data = pd.read_csv('California_CMAQ_PM2.5_Output_all.csv', \\\n","                         usecols = ['Date','Latitude', 'Longitude', 'PM'])\n","\n","# extracts data from chosen date\n","data_start = 0\n","data_end = 0\n","while CMAQ_data.Date[data_start] != date:\n","    data_start += 1\n","data_end = data_start\n","while CMAQ_data.Date[data_end] == date:\n","    data_end += 1\n","    \n","CMAQ_data = CMAQ_data.drop(list(range(data_end+1,673251)))\n","CMAQ_data = CMAQ_data.drop(list(range(0,data_start)))\n","\n","# organizes data further\n","CMAQ_data = {'Lat': CMAQ_data.Latitude,\n","             'Long': CMAQ_data.Longitude,\n","             'PM': CMAQ_data.PM}\n","\n","# saves PM data length (this will be important soon)\n","PM_data_length = len(CMAQ_data['PM'])\n","\n","\"\"\" FOR TESTING\n","print(PM_data_length)\n","print(len(CMAQ_data['Long']))\n","print(len(CMAQ_data['Lat']))\n","print(CMAQ_data['Long'][j])\n","\"\"\"\n","\n","# and even further\n","CMAQ_coords = list(range(PM_data_length-1))\n","for a in range(data_start, data_end):\n","    CMAQ_coords[a-data_start] = [CMAQ_data['Long'][a], CMAQ_data['Lat'][a]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mmlzuxT4mKDH"},"source":["We need to bind each site to a CMAQ sampling site. First, we must find the coordinates of each site. Then we can find each site's matching CMAQ sampling site. We do this in the slightly terrible method seen below."]},{"cell_type":"code","metadata":{"id":"FVkDaEBfmKDH"},"source":["# lat: 35.0, 38.2, diff 3.2\n","# long: -121.7, -117.79, diff 3.91'\n","for y in range(w_h):\n","    for x in range(w_h):\n","\n","        # assign longitudes and latitudes to each site\n","        coords_of_sites[x + w_h*y] = [-117.79 - x * (0.011189 * resolution), 35 + y * (.008994 * resolution)]\n","\n","with open(\"coords_\" + str(resolution) + \".npy\", 'wb') as f:\n","    np.save(f, coords_of_sites)# allow_pickle=False)\n","\n","\n","# locates nearest CMAQ sampling site, replaces coords_toCMAQ lat/long data with PM2.5 data \n","# from this nearest sampling site.\n","for y in range(w_h):\n","    #progressBar(y, w_h, barLength = 80)\n","    for x in range(w_h):\n","        distances = []\n","        distances = cdist([coords_of_sites[x + w_h*y]], CMAQ_coords, 'euclidean')\n","        CMAQ_offset[x + w_h*y] = CMAQ_data['PM'][np.argmin(distances) + data_start]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAsk7wAzmKDH"},"source":["Finally, we can put all the pieces together."]},{"cell_type":"code","metadata":{"id":"DBggsyRPmKDI"},"source":["# generates noise\n","noise = np.random.normal(0, 0.10, s)\n","\n","# final model\n","for i in range(s):\n","    data[i] = (beta_0 + A_11 * sample[i] + beta_1 * math.sqrt(CMAQ_offset[i]) + noise[i])**2\n","\n","# saves data\n","with open(\"data_\" + str(resolution) + \".npy\", 'wb') as f:\n","        np.save(f, data, allow_pickle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2bOZiUXmKDI","outputId":"263ae814-bb80-41ac-bdd0-99d98162e22c"},"source":["print(len(data))\n","print(data[0:5])\n","print(data[len(data)-6:len(data)-1])\n","print(CMAQ_data['PM'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7921\n","[9.756278110267564, 9.16114609157784, 10.691859891655437, 9.4359381689295, 8.82159245501556]\n","[24.0187866409146, 24.097423935183123, 24.17431505685213, 20.621372842767535, 21.02625270110925]\n","335244     5.7133\n","335245     6.0214\n","335246     6.0344\n","335247     8.2972\n","335248     8.4503\n","           ...   \n","336161    10.2465\n","336162    22.0778\n","336163    38.2411\n","336164    14.0042\n","336165     6.2717\n","Name: PM, Length: 922, dtype: float64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zCgIxWlWmKDI"},"source":["Now we need to upload and clean the monitoring station data."]},{"cell_type":"code","metadata":{"id":"EKKE0q1BmKDJ"},"source":["# uploads monitoring station data\n","PM_data = pd.read_csv('California_MonitoringStations_PM2.5_SimplifiedOutput.csv', \\\n","                         usecols = ['Date','Latitude', 'Longitude', 'PM'])\n","\n","# finds data for relevant date\n","PM_lat = []\n","PM_long = []\n","PM_PM = []\n","for i in range(len(PM_data.Date)):\n","    if PM_data.Date[i] == date:\n","        PM_PM.append(PM_data.PM[i])\n","        PM_long.append(PM_data.Longitude[i])\n","        PM_lat.append(PM_data.Latitude[i])\n","\n","# saves PM data length, we will need this later\n","PM_data_length = len(PM_lat)        \n","        \n","# creates array of latlong data, we will need this later\n","PM_coords = list(range(PM_data_length))\n","for i in range(PM_data_length):\n","    PM_coords[i] = [[PM_long[i], PM_lat[i]]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DeLSHV9XmKDJ"},"source":["Finally, we can perform"]},{"cell_type":"code","metadata":{"id":"uhG80epkmKDJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rD3Zvj_tmKDJ","outputId":"658eeffb-3a25-4c53-b6c2-b6e598ff6268"},"source":["PM_CMAQ_diff = []\n","PM_downscaler_diff = []\n","\n","for i in range(1):# PM_data_length):\n","    \n","    distances = []\n","    \n","    #print(cdist(PM_coords[i], CMAQ_coords, 'euclidean'))\n","    \n","    distances = cdist(PM_coords[i], CMAQ_coords, 'euclidean')\n","    \n","    print(CMAQ_data['Lat'][np.argmin(distances) + data_start])\n","    print(CMAQ_data['Long'][np.argmin(distances) + data_start])\n","    print(CMAQ_data['PM'][np.argmin(distances) + data_start] - PM_PM[i])\n","    \n","    PM_CMAQ_diff.append(CMAQ_data['PM'][np.argmin(distances) + data_start] - PM_PM[i])\n","    \n","    \n","    \n","    distances = []\n","    \n","    #print(cdist(PM_coords[i], coords_of_sites, 'euclidean'))\n","    \n","    distances = cdist(PM_coords[i], coords_of_sites, 'euclidean')\n","    \n","    \n","    print(coords_of_sites[np.argmin(distances)])\n","    print(data[np.argmin(distances)] - PM_PM[i])\n","    \n","    PM_downscaler_diff.append(data[np.argmin(distances)] - PM_PM[i])\n","\n","MSE_CMAQ = (np.square(PM_CMAQ_diff)).mean()\n","MSE_downscaler = (np.square(PM_downscaler_diff)).mean()\n","\n","print(MSE_CMAQ)\n","print(MSE_downscaler)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["36.78691\n","-119.78145\n","-0.23810000000000286\n","[-119.759264, 36.7988]\n","-24.434525521493658\n","0.05669161000000136\n","597.0460374605249\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YiYWWtVrmKDK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vA0wBYD-mKDK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3un_wo6mKDK"},"source":[""],"execution_count":null,"outputs":[]}]}